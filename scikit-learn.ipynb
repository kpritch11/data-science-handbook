{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Scikit Learn\n",
    "\n",
    "#### 5.1 Machine Learning\n",
    "\n",
    "**Machine learning** involves building mathematical models to help\n",
    "understand data. “Learning” enters the fray when we give these models tunable\n",
    "parameters that can be adapted to observed data; in this way the program can be con‐\n",
    "sidered to be “learning” from the data. Once these models have been fit to previously\n",
    "seen data, they can be used to predict and understand aspects of newly observed data.\n",
    "\n",
    "##### Categories of Machine Learning\n",
    "\n",
    "**Supervised learning** involves somehow modeling the relationship between measured\n",
    "features of data and some label associated with the data; once this model is deter‐\n",
    "mined, it can be used to apply labels to new, unknown data. This is further subdivided \n",
    "into **classification tasks** and **regression tasks**: in classification, the labels are\n",
    "discrete categories, while in regression, the labels are continuous quantities. We will\n",
    "see examples of both types of supervised learning in the following section.\n",
    "\n",
    "**Unsupervised learning** involves modeling the features of a dataset without reference to\n",
    "any label, and is often described as “letting the dataset speak for itself.” These models\n",
    "include tasks such as clustering and dimensionality reduction. **Clustering algorithms** \n",
    "identify distinct groups of data, while **dimensionality reduction algorithms** search for\n",
    "more succinct representations of the data. We will see examples of both types of\n",
    "unsupervised learning in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Introducing Scikit-Learn\n",
    "\n",
    "##### Data Representation in Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data as a table\n",
    "import seaborn as sns\n",
    "iris = sns.load_dataset('iris')\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features Matrix** is a two-dimensional numerical array or matrix, with shape [n_samples, n_features], \n",
    "and is most often contained in a NumPy array or Pandas DataFrame, and stored in variable X.\n",
    "\n",
    "**Samples** (i.e., rows) always refer to the individual objects described by the dataset.\n",
    "For example, the sample might be a flower, a person, a document, an image, a sound\n",
    "file, a video, an astronomical object, or anything else you can describe with a set of\n",
    "quantitative measurements.\n",
    "\n",
    "**Features** (i.e., columns) always refer to the distinct observations that describe\n",
    "each sample in a quantitative manner. Features are generally real-valued, but may be\n",
    "Boolean or discrete-valued in some cases.\n",
    "\n",
    "**Label or Target Array** is one dimensional, with length n_samples, \n",
    "and is most often contained in a NumPy arrary or Pandas Series,\n",
    "and stored in variable y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns; sns.set()\n",
    "sns.pairplot(iris, hue='species', height=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_iris = iris.drop('species', axis=1)\n",
    "X_iris.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_iris = iris['species']\n",
    "y_iris.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scikit-Learn's Estimator API\n",
    "\n",
    "Most commonly, the steps in using the Scikit-Learn estimator API are as follows (we\n",
    "will step through a handful of detailed examples in the sections that follow):\n",
    "\n",
    "1. Choose a class of model by importing the appropriate estimator class from Scikit-\n",
    "Learn.\n",
    "2. Choose model hyperparameters by instantiating this class with desired values.\n",
    "3. Arrange data into a features matrix and target vector following the discussion\n",
    "from before.\n",
    "4. Fit the model to your data by calling the fit() method of the model instance.\n",
    "5. Apply the model to new data:\n",
    "    - For supervised learning, often we predict labels for unknown data using the\n",
    "predict() method.\n",
    "    - For unsupervised learning, we often transform or infer properties of the data\n",
    "using the transform() or predict() method.\n",
    "\n",
    "##### Ex. Supervised Learning: Simple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# scatter plot\n",
    "rng = np.random.RandomState(42)\n",
    "x = 10 * rng.rand(50)\n",
    "y = 2 * x - 1 + rng.randn(50)\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. choose a class of model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 2. choose model hyperparameters\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "\n",
    "# 3. arrange data into features matrix and target vector\n",
    "X = x[:, np.newaxis]\n",
    "\n",
    "# 4. fit model to data\n",
    "model.fit(X, y)\n",
    "\n",
    "# 5. predict labels for unknown data\n",
    "xfit = np.linspace(-1, 11)\n",
    "\n",
    "# transform data to fit into model\n",
    "Xfit = xfit[:, np.newaxis]\n",
    "yfit = model.predict(Xfit)\n",
    "\n",
    "# visualize the results\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, yfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ex. Supervised Learning: Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X_iris, y_iris, random_state=1)\n",
    "\n",
    "# 1. choose model class\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# 2. instantiate model\n",
    "model = GaussianNB()\n",
    "\n",
    "# 3. fit model to data\n",
    "model.fit(Xtrain, ytrain)\n",
    "\n",
    "# 4. predict on new data\n",
    "y_model = model.predict(Xtest)\n",
    "\n",
    "# check accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(ytest, y_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ex. Unsupervised Learning: Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. choose the model class\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 2. instantiate the model with hyperparameters\n",
    "model = PCA(n_components=2)\n",
    "\n",
    "# 3. fit to data\n",
    "model.fit(X_iris)\n",
    "\n",
    "# 4. transform the data to two dimensions\n",
    "X_2D = model.transform(X_iris)\n",
    "\n",
    "# plot the results\n",
    "iris['PCA1'] = X_2D[:, 0]\n",
    "iris['PCA2'] = X_2D[:, 1]\n",
    "sns.lmplot(x='PCA1', y='PCA2', hue='species', data=iris, fit_reg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ex. Unsupervised Learning: Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. choose the model class\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# 2. instantiate the model with hyperparameters\n",
    "model = GaussianMixture(n_components=3, covariance_type='full')\n",
    "\n",
    "# 3. fit to data\n",
    "model.fit(X_iris)\n",
    "\n",
    "# 4. determine cluster labels\n",
    "y_gmm = model.predict(X_iris)\n",
    "\n",
    "# plot the results\n",
    "iris['cluster'] = y_gmm\n",
    "sns.lmplot(x='PCA1', y='PCA2', data=iris, hue='species', col='cluster', fit_reg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Application: Exploring Handwritten Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load digits data\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.images.shape\n",
    "\n",
    "# visualize the first hundred of these\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(10, 10, figsize=(8, 8), subplot_kw={'xticks':[], 'yticks':[]}, gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(digits.images[i], cmap='binary', interpolation='nearest')\n",
    "    ax.text(0.05, 0.05, str(digits.target[i]), transform=ax.transAxes, color='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = digits.data\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = digits.target\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# unsupervised learning: dimensionality reduction\n",
    "from sklearn.manifold import Isomap\n",
    "iso = Isomap(n_components=2)\n",
    "iso.fit(digits.data)\n",
    "data_projected = iso.transform(digits.data)\n",
    "data_projected.shape\n",
    "\n",
    "# plot the results\n",
    "plt.scatter(data_projected[:, 0], data_projected[:, 1], c=digits.target, edgecolor='none', alpha=0.5, cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
    "plt.colorbar(label='digit label', ticks=range(10))\n",
    "plt.clim(-0.5, 9.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification on digits\n",
    "\n",
    "# split data into training and testing sets\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# 1. choose model class\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# 2. instantiate model\n",
    "model = GaussianNB()\n",
    "\n",
    "# 3. fit model to data\n",
    "model.fit(Xtrain, ytrain)\n",
    "\n",
    "# 4. predict on new data\n",
    "y_model = model.predict(Xtest)\n",
    "\n",
    "# check accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(ytest, y_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "mat = confusion_matrix(ytest, y_model)\n",
    "\n",
    "sns.heatmap(mat, square=True, annot=True, cbar=False)\n",
    "plt.xlabel('predicted value')\n",
    "plt.ylabel('true value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the digits with the predicted labels\n",
    "\n",
    "fig, axes = plt.subplots(10, 10, figsize=(8, 8), subplot_kw={'xticks':[], 'yticks':[]}, gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(digits.images[i], cmap='binary', interpolation='nearest')\n",
    "    ax.text(0.05, 0.05, str(y_model[i]), transform=ax.transAxes, color='green' if (ytest[i] == y_model[i]) else 'red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Hyperparameters and Model Validation\n",
    "\n",
    "##### Thinking About Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model validation the wrong way\n",
    "\n",
    "# load data\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# choose a model and hyperparameters\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "# train the model\n",
    "model.fit(X, y)\n",
    "y_model = model.predict(X)\n",
    "\n",
    "# check the accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y, y_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model validation the right way: holdout sets\n",
    "\n",
    "# load data and split 50/50\n",
    "from sklearn.model_selection import train_test_split\n",
    "X1, X2, y1, y2 = train_test_split(X, y, random_state=0, train_size=0.5)\n",
    "\n",
    "# fit the model on one set of data\n",
    "model.fit(X1, y1)\n",
    "\n",
    "# evaluate the model on the second set of data\n",
    "y2_model = model.predict(X2)\n",
    "accuracy_score(y2, y2_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Validation** is a sequence of fits where each subset of the data is used both as a training set and as a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model validation via cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(model, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leave-one-out cross-validation\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "scores = cross_val_score(model, X, y, cv=LeaveOneOut())\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Selecting the Best Model\n",
    "\n",
    "If our estimator is underperforming, how should we move forward?\n",
    "- Use a more complicated/more flexible model\n",
    "- Use a less complicated/less flexible model\n",
    "- Gather more training samples\n",
    "- Gather more data to add features to each sample\n",
    "\n",
    "##### Bias-Variance Trade off\n",
    "\n",
    "The best model is optimized in the trade off of bias and variance, \n",
    "where **bias** means the model underfits the data, \n",
    "and **variance** means the model overfits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation curves\n",
    "\n",
    "# linear regression and polynomial preprocessor and pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree), LinearRegression(**kwargs))\n",
    "\n",
    "# create some data\n",
    "import numpy as np\n",
    "\n",
    "def make_data(N, err=1.0, rseed=1):\n",
    "    # randomly sample the data\n",
    "    rng = np.random.RandomState(rseed)\n",
    "    X = rng.rand(N, 1) ** 2\n",
    "    y = 10 - 1./(X.ravel() + 0.1)\n",
    "    if err > 0:\n",
    "        y += err * rng.randn(N)\n",
    "    return X, y\n",
    "\n",
    "X, y = make_data(40)\n",
    "\n",
    "# visualize the data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; seaborn.set() # plot formatting\n",
    "%matplotlib inline\n",
    "\n",
    "X_test = np.linspace(-0.1, 1.1, 500)[:, None]\n",
    "\n",
    "plt.scatter(X.ravel(), y, color='black')\n",
    "axis = plt.axis()\n",
    "for degree in [1, 3, 5]:\n",
    "    y_test = PolynomialRegression(degree).fit(X, y).predict(X_test)\n",
    "    plt.plot(X_test.ravel(), y_test, label='degree={0}'.format(degree))\n",
    "plt.xlim(-0.1, 1.0)\n",
    "plt.ylim(-2, 12)\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation curves in scikit-learn\n",
    "\n",
    "from sklearn.model_selection import validation_curve\n",
    "degree = np.arange(0, 21)\n",
    "train_score, val_score = validation_curve(PolynomialRegression(), X, y, param_name='polynomialfeatures__degree', param_range=degree, cv=7)\n",
    "\n",
    "plt.plot(degree, np.median(train_score, 1), color='blue', label='training score')\n",
    "plt.plot(degree, np.median(val_score, 1), color='red', label='validation score')\n",
    "plt.legend(loc='best')\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from above: optimal model is given by degree=3\n",
    "# compute and display this fit over the original data\n",
    "\n",
    "plt.scatter(X.ravel(), y)\n",
    "lim = plt.axis()\n",
    "y_test = PolynomialRegression(3).fit(X, y).predict(X_test)\n",
    "plt.plot(X_test.ravel(), y_test)\n",
    "plt.axis(lim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** that finding the optimal model did not actually require us to compute the training score, \n",
    "instead we examined the relationship between the training score and validation score.\n",
    "\n",
    "<img src=\"validation-curve.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data\n",
    "X2, y2 = make_data(200)\n",
    "plt.scatter(X2.ravel(), y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot validation curve for new data\n",
    "\n",
    "degree = np.arange(21)\n",
    "train_score2, val_score2 = validation_curve(PolynomialRegression(), X2, y2, param_name='polynomialfeatures__degree', param_range=degree, cv=7)\n",
    "\n",
    "plt.plot(degree, np.median(train_score2, 1), color='blue', label='training score')\n",
    "plt.plot(degree, np.median(val_score2, 1), color='red', label='validation score')\n",
    "plt.plot(degree, np.median(train_score, 1), color='blue', alpha=0.3, linestyle='dashed')\n",
    "plt.plot(degree, np.median(val_score, 1), color='red', alpha=0.3, linestyle='dashed')\n",
    "plt.legend(loc='lower center')\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning curve** is the plot of the training/validation score with respect to the size of the training set.\n",
    "- a model of given complexity will *overfit* a small dataset: high training score, low validation score\n",
    "- a model of given complexity will *underfit* a large dataset: will decrease training score, increase validation score\n",
    "- a model will never (except by chance) give a better score to the validation set than the training set, \n",
    "this means the curves should keep getting closer together but never cross."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"learning-curve.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning curves in scikit-learn\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "for i, degree in enumerate([2, 9]):\n",
    "    N, train_lc, val_lc = learning_curve(PolynomialRegression(degree), X, y, cv=7, train_sizes=np.linspace(0.3, 1, 25))\n",
    "    \n",
    "    ax[i].plot(N, np.mean(train_lc, 1), color='blue', label='training score')\n",
    "    ax[i].plot(N, np.mean(val_lc, 1), color='red', label='validation score')\n",
    "    ax[i].hlines(np.mean([train_lc[-1], val_lc[-1]]), N[0], N[-1], color='gray', linestyle='dashed')\n",
    "    \n",
    "    ax[i].set_ylim(0, 1)\n",
    "    ax[i].set_xlim(N[0], N[-1])\n",
    "    ax[i].set_xlabel('training size')\n",
    "    ax[i].set_ylabel('score')\n",
    "    ax[i].set_title('degree = {0}'.format(degree), size=14)\n",
    "    ax[i].legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation in Practice: Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'polynomialfeatures__degree': np.arange(21), \n",
    "              'linearregression__fit_intercept': [True, False]}\n",
    "\n",
    "grid = GridSearchCV(PolynomialRegression(), param_grid, cv=7)\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the best fit model determined via an automatic grid-search\n",
    "\n",
    "model = grid.best_estimator_\n",
    "\n",
    "plt.scatter(X.ravel(), y)\n",
    "lim = plt.axis()\n",
    "y_test = model.fit(X, y).predict(X_test)\n",
    "plt.plot(X_test.ravel(), y_test)\n",
    "plt.axis(lim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 Feature Engineering\n",
    "\n",
    "**Feature Engineering** is taking whatever information you have about your probelm and turning it into numbers that you can use to build your feature matrix.\n",
    "\n",
    "##### Categorical Features\n",
    "\n",
    "For **nominal** categorical variables, use **one-hot encoding**  \n",
    "For **ordinal** categorical variables, use a numeric mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data\n",
    "data = [\n",
    "    {'price': 850000, 'rooms': 4, 'neighborhood': 'Queen Anne'},\n",
    "    {'price': 700000, 'rooms': 3, 'neighborhood': 'Fremont'},\n",
    "    {'price': 650000, 'rooms': 3, 'neighborhood': 'Wallingford'},\n",
    "    {'price': 600000, 'rooms': 2, 'neighborhood': 'Fremont'}\n",
    "]\n",
    "\n",
    "# straightforward numerical mapping\n",
    "{'Queen Anne': 1, 'Fremont': 2, 'Wallingford': 3}\n",
    "\n",
    "# one hot encoding using scikit learn\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vec = DictVectorizer(sparse=False, dtype=int)\n",
    "vec.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse output\n",
    "vec = DictVectorizer(sparse=True, dtype=int)\n",
    "vec.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = ['problem of evil', 'evil queen', 'horizon problem']\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(sample)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(X.toarray(), columns=vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# term frequency-inverse document frequency (TF-IDF)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vec = TfidfVectorizer()\n",
    "X = vec.fit_transform(sample)\n",
    "pd.DataFrame(X.toarray(), columns=vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Image Features\n",
    "\n",
    "The simplest approach is what we used for the digits data by simply using the pixel values themselves.\n",
    "\n",
    "##### Derived Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data that is not well described by a straight line\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([4, 2, 1, 3, 7])\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poor fit straight line\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X = x[:, np.newaxis]\n",
    "model = LinearRegression().fit(X, y)\n",
    "yfit = model.predict(X)\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, yfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add polynomial features to the data\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X2 = poly.fit_transform(X)\n",
    "print(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute linear regression on the transformed features\n",
    "model = LinearRegression().fit(X2, y)\n",
    "yfit = model.predict(X2)\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, yfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This idea of improving a model not by changing the model, but by transforming the inputs, is fundamental to many of the more powerful machine learning methods.\n",
    "\n",
    "##### Imputation of Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import nan\n",
    "X = np.array([[ nan, 0, 3 ],\n",
    "                [ 3, 7, 9 ],\n",
    "                [ 3, 5, 2 ],\n",
    "                [ 4, nan, 6 ],\n",
    "                [ 8, 8, 1 ]])\n",
    "y = np.array([14, 16, -1,  8, -5])\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(strategy='mean')\n",
    "X2 = imp.fit_transform(X)\n",
    "X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(X2, y)\n",
    "model.predict(X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Pipelines\n",
    "\n",
    "- Impute the missing values using the mean\n",
    "- Transform features to quadratic\n",
    "- Fit a linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(\n",
    "    SimpleImputer(strategy='mean'), \n",
    "    PolynomialFeatures(degree=2), \n",
    "    LinearRegression()\n",
    ")\n",
    "\n",
    "model.fit(X, y)\n",
    "print(y)\n",
    "print(model.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 In Depth: Naive Bayes Classification\n",
    "\n",
    "**Naive Bayes** models are a group of extremely fast and simple classification algorithms that are often suitable for very high dimensional datasets.\n",
    "\n",
    "##### Bayesian Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(n_samples=100, centers=2, random_state=2, cluster_std=1.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One extremely fast way to create a simple model is to assume that the data is described by a Gaussian distribution with no covariance between dimensions.\n",
    "We can fit this model by simply finding the mean and standard deviation of the points within each label, which is all you need to define such a distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(X, y)\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "Xnew = [-6, -14] + [14, 18] * rng.rand(2000, 2)\n",
    "ynew = model.predict(Xnew)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu')\n",
    "lim = plt.axis()\n",
    "plt.scatter(Xnew[:, 0], Xnew[:, 1], c=ynew, s=20, cmap='RdBu', alpha=0.1)\n",
    "plt.axis(lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yprob = model.predict_proba(Xnew)\n",
    "yprob[-8:].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multinomial Naive Bayes\n",
    "\n",
    "**Multinomial distribution** describes the probability of observing counts amoung a number of categories, \n",
    "and thus multinomial naive bayes is most appropriate for features that represent counts or count rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex. Classifying Text\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "data = fetch_20newsgroups()\n",
    "data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['talk.religion.misc', 'soc.religion.christian', 'sci.space', 'comp.graphics']\n",
    "train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "test = fetch_20newsgroups(subset='test', categories=categories)\n",
    "print(train.data[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert text to a vector using TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "\n",
    "# apply the model to the training data, predict labels for the test data\n",
    "model.fit(train.data, train.target)\n",
    "labels = model.predict(test.data)\n",
    "\n",
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(test.target, labels)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=train.target_names, yticklabels=train.target_names)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict category on new data\n",
    "def predict_category(s, train=train, model=model):\n",
    "    pred = model.predict([s])\n",
    "    return train.target_names[pred[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_category('sending a payload to the ISS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_category('discussing islam vs atheism')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_category('determining the screen resolution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### When to Use Naive Bayes\n",
    "\n",
    "Advantages of bayesian classifiers:\n",
    "- They are extremely fast for both training and prediction\n",
    "- They provide straightforward probabilistic prediction\n",
    "- They are often very easily interpretable\n",
    "- They have very few (if any) turnable parameters\n",
    "\n",
    "Perform well when:\n",
    "- When the naive assumptions actually match the data (very rare in practice)\n",
    "- For very well separated categories, when model complexity is less important\n",
    "- For very high dimensional data, when model complexity is less important\n",
    "\n",
    "For this reason, simple classifiers like naive bayes tend to work well or better than more complicated classifiers as the dimensionality grows: \n",
    "once you have enough data, even a simple model can be very powerful.\n",
    "\n",
    "#### 5.6 In Depth: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for linear regression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "x = 10 * rng.rand(50)\n",
    "y = 2 * x - 5 + rng.randn(50)\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "\n",
    "model.fit(x[:, np.newaxis], y)\n",
    "\n",
    "xfit = np.linspace(0, 10, 1000)\n",
    "yfit = model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, yfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model slope:    ', model.coef_[0])\n",
    "print('Model intercept:', model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "X = 10 * rng.rand(100, 3)\n",
    "y = 0.5 + np.dot(X, [1.5, -2., 1.])\n",
    "\n",
    "model.fit(X, y)\n",
    "print(model.intercept_)\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
